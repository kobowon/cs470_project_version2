{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "# import torchvision.transforms as transforms\n",
    "from data.load_datasets import *\n",
    "from experiment.utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from nlgeval import NLGEval\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_folder = 'preprocessed_dataset'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'preprocessed_coco'  # base name shared by data files\n",
    "checkpoint_file = 'BEST_34checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Load model\n",
    "torch.nn.Module.dump_patches = True\n",
    "checkpoint = torch.load(checkpoint_file, map_location = device)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "\n",
    "nlgeval = NLGEval()  # loads the evaluator\n",
    "\n",
    "#BERT Tokenizer\n",
    "model_name_or_path = \"bert-base-uncased\" \n",
    "tokenizer_class = BertTokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case = True)\n",
    "\n",
    "\n",
    "#token to id\n",
    "#tokenizer._convert_token_to_id(token)\n",
    "\n",
    "#id to token\n",
    "#tokenizer._convert_id_to_token(index)\n",
    "\n",
    "#word_map_file\n",
    "#word_map\n",
    "#rev_word_map\n",
    "\n",
    "BERT_VOCA_SIZE = 30522\n",
    "vocab_size = BERT_VOCA_SIZE #len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation\n",
    ":param beam_size: beam size at which to generate captions for evaluation\n",
    ":return: Official MSCOCO evaluator scores - bleu4, cider, rouge, meteor\n",
    "\"\"\"\n",
    "# DataLoader\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    CaptionDataset(data_folder, data_name, 'TEST'),\n",
    "    batch_size=1, shuffle=True, num_workers=1, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "# If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "# references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "references = list()\n",
    "hypotheses = list()\n",
    "\n",
    "CLS_IDX = 101\n",
    "SEP_IDX = 102\n",
    "PAD_IDX = 0\n",
    "start_idx = CLS_IDX\n",
    "end_idx = SEP_IDX\n",
    "\n",
    "# For each image\n",
    "for i, (image_features, caps, caplens, allcaps) in enumerate(\n",
    "        tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "\n",
    "    k = beam_size\n",
    "\n",
    "    # Move to GPU device, if available\n",
    "    image_features = image_features.to(device)  # (1, 3, 256, 256)\n",
    "    image_features_mean = image_features.mean(1)\n",
    "    image_features_mean = image_features_mean.expand(k,2048)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    CLS_IDX = 101\n",
    "    start_idx = CLS_IDX\n",
    "    k_prev_words = torch.LongTensor([[start_idx]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Lists to store completed sequences and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h1, c1 = decoder.init_hidden_state(k)  # (batch_size, decoder_dim)\n",
    "    h2, c2 = decoder.init_hidden_state(k)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "        h1,c1 = decoder.top_down_attention(\n",
    "            torch.cat([h2,image_features_mean,embeddings], dim=1),\n",
    "            (h1,c1))  # (batch_size_t, decoder_dim)\n",
    "        attention_weighted_encoding = decoder.attention(image_features,h1)\n",
    "        h2,c2 = decoder.language_model(\n",
    "            torch.cat([attention_weighted_encoding,h1], dim=1),(h2,c2))\n",
    "\n",
    "        scores = decoder.fc(h2)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # Add new words to sequences\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != end_idx]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h1 = h1[prev_word_inds[incomplete_inds]]\n",
    "        c1 = c1[prev_word_inds[incomplete_inds]]\n",
    "        h2 = h2[prev_word_inds[incomplete_inds]]\n",
    "        c2 = c2[prev_word_inds[incomplete_inds]]\n",
    "        image_features_mean = image_features_mean[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "\n",
    "#     CLS_IDX = 101\n",
    "# SEP_IDX = 102\n",
    "# PAD_IDX = 0\n",
    "# start_idx = CLS_IDX\n",
    "# end_idx = SEP_IDX\n",
    "#tokenizer._convert_id_to_token(102)\n",
    "    # References\n",
    "    #원래꺼가 [[]]이런식으로 되어있었나?, 변경 > [[],[],[],[]]\n",
    "    img_caps = allcaps[0].tolist()\n",
    "    img_captions = list(\n",
    "        map(lambda c: [tokenizer._convert_id_to_token(w) for w in c if w not in {start_idx, end_idx, PAD_IDX}],\n",
    "            img_caps))  # remove <start> and pads\n",
    "    img_caps = [' '.join(c) for c in img_captions]\n",
    "    #print(img_caps)\n",
    "    references.append(img_caps)\n",
    "\n",
    "    # Hypotheses\n",
    "    hypothesis = ([tokenizer._convert_id_to_token(w) for w in seq if w not in {start_idx, end_idx, PAD_IDX}])\n",
    "    hypothesis = ' '.join(hypothesis)\n",
    "    #print(hypothesis)\n",
    "    hypotheses.append(hypothesis)\n",
    "    assert len(references) == len(hypotheses)\n",
    "\n",
    "# Calculate scores\n",
    "metrics_dict = nlgeval.compute_metrics(references, hypotheses)\n",
    "print(metrics_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
